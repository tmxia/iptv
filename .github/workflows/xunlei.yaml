name: Xunlei Download Automation
on: [workflow_dispatch]

jobs:
  download:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          pip install cloudscraper requests beautifulsoup4 tqdm
          
      - name: Run robust Xunlei downloader
        env:
          XUNLEI_URL: "https://pan.xunlei.com/s/VOOlHjZarK69yZ7BxcLd1WrsA1?pwd=ckit"
        run: |
          cat <<EOF > xunlei_downloader.py
          import os
          import re
          import json
          import cloudscraper
          import requests
          from bs4 import BeautifulSoup
          from tqdm import tqdm
          from urllib.parse import urlparse, parse_qs, quote
          
          # 配置参数
          XUNLEI_URL = os.environ.get('XUNLEI_URL', '')
          REFERER = "https://pan.xunlei.com/"
          USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
          
          def extract_share_id(url):
              """从分享链接中提取share_id"""
              parsed = urlparse(url)
              path_segments = parsed.path.split('/')
              if len(path_segments) > 2 and path_segments[2]:
                  return path_segments[2]
              return None
          
          def extract_password(url):
              """从URL中提取密码"""
              parsed = urlparse(url)
              query = parse_qs(parsed.query)
              return query.get('pwd', [None])[0]
          
          def get_cf_scraper():
              """创建Cloudflare绕过器"""
              return cloudscraper.create_scraper(
                  browser={
                      'browser': 'chrome',
                      'platform': 'windows',
                      'desktop': True
                  }
              )
          
          def get_share_page_content(url):
              """获取分享页面内容"""
              scraper = get_cf_scraper()
              headers = {
                  "User-Agent": USER_AGENT,
                  "Referer": REFERER,
                  "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                  "Accept-Language": "en-US,en;q=0.5",
                  "Connection": "keep-alive",
                  "Upgrade-Insecure-Requests": "1",
              }
              
              print("绕过Cloudflare防护中...")
              response = scraper.get(url, headers=headers)
              response.raise_for_status()
              
              # 检查是否被重定向到验证页面
              if "verify" in response.url:
                  print("检测到验证页面，尝试自动处理...")
                  return handle_verification(scraper, response.url)
              
              return response.text, scraper.cookies.get_dict()
          
          def handle_verification(scraper, verify_url):
              """处理验证页面"""
              # 获取验证页面内容
              response = scraper.get(verify_url)
              soup = BeautifulSoup(response.text, 'html.parser')
              
              # 查找验证表单
              form = soup.find('form', id='challenge-form')
              if not form:
                  raise Exception("无法找到验证表单")
              
              # 提取表单数据
              inputs = form.find_all('input')
              form_data = {}
              for input_tag in inputs:
                  if input_tag.get('name'):
                      form_data[input_tag['name']] = input_tag.get('value', '')
              
              # 提交验证表单
              action_url = form.get('action')
              if not action_url.startswith('http'):
                  action_url = f"https://{urlparse(verify_url).netloc}{action_url}"
              
              print("提交验证表单...")
              response = scraper.post(action_url, data=form_data)
              response.raise_for_status()
              
              return response.text, scraper.cookies.get_dict()
          
          def parse_file_info(html_content):
              """从HTML内容中解析文件信息"""
              soup = BeautifulSoup(html_content, 'html.parser')
              
              # 查找文件列表容器
              file_list_container = soup.find('div', class_='file-list')
              if not file_list_container:
                  raise Exception("无法找到文件列表")
              
              # 提取文件信息
              file_items = file_list_container.find_all('div', class_='file-item')
              files = []
              
              for item in file_items:
                  file_name = item.find('span', class_='file-name').text.strip()
                  file_id = item.get('data-file-id')
                  
                  if not file_id:
                      continue
                  
                  # 提取文件大小
                  size_element = item.find('span', class_='file-size')
                  file_size = size_element.text.strip() if size_element else "未知大小"
                  
                  files.append({
                      'name': file_name,
                      'file_id': file_id,
                      'size': file_size
                  })
              
              return files
          
          def get_download_token(scraper, cookies, file_id):
              """获取下载token"""
              token_url = "https://pan.xunlei.com/api/v1/share/download/token"
              headers = {
                  "User-Agent": USER_AGENT,
                  "Referer": REFERER,
                  "X-Requested-With": "XMLHttpRequest",
                  "Content-Type": "application/json"
              }
              
              payload = {
                  "file_id": file_id,
                  "nonce": os.urandom(8).hex()
              }
              
              response = scraper.post(token_url, json=payload, headers=headers, cookies=cookies)
              response.raise_for_status()
              
              result = response.json()
              if result.get("code") != 0:
                  raise Exception(f"获取下载token失败: {result.get('message', '未知错误')}")
              
              return result["data"]["token"]
          
          def construct_download_url(file_name, token):
              """构建下载URL"""
              # 迅雷下载URL格式：https://down.sandai.net/thunder/[文件名]?token=[token]
              encoded_name = quote(file_name.encode('utf-8'))
              return f"https://down.sandai.net/thunder/{encoded_name}?token={token}"
          
          def download_file(url, filename, cookies=None):
              """下载文件并显示进度条"""
              headers = {
                  "User-Agent": USER_AGENT,
                  "Referer": REFERER
              }
              
              # 创建会话以保持cookie
              session = requests.Session()
              if cookies:
                  session.cookies.update(cookies)
              
              # 获取文件大小
              response = session.head(url, headers=headers, allow_redirects=True)
              file_size = int(response.headers.get('Content-Length', 0))
              
              # 下载文件
              response = session.get(url, headers=headers, stream=True)
              response.raise_for_status()
              
              # 创建进度条
              progress = tqdm(total=file_size, unit='B', unit_scale=True, desc=filename)
              
              # 写入文件
              with open(filename, 'wb') as f:
                  for chunk in response.iter_content(chunk_size=8192):
                      if chunk:
                          f.write(chunk)
                          progress.update(len(chunk))
              progress.close()
              
              print(f"文件下载完成: {filename}")
          
          def main():
              print("=== 迅雷网盘下载自动化 ===")
              print(f"目标分享链接: {XUNLEI_URL}")
              
              # 提取分享ID和密码
              share_id = extract_share_id(XUNLEI_URL)
              password = extract_password(XUNLEI_URL)
              
              if not share_id:
                  raise ValueError("无法从URL中提取share_id")
              
              print(f"分享ID: {share_id}")
              print(f"提取密码: {password or '无密码'}")
              
              # 构建带密码的URL
              target_url = f"https://pan.xunlei.com/s/{share_id}"
              if password:
                  target_url += f"?pwd={password}"
              
              # 获取分享页面内容
              html_content, cookies = get_share_page_content(target_url)
              
              # 解析文件信息
              print("解析文件列表...")
              files = parse_file_info(html_content)
              
              if not files:
                  raise Exception("未找到可下载文件")
              
              print("\n分享包含的文件:")
              for idx, file_info in enumerate(files):
                  print(f"{idx+1}. {file_info['name']} ({file_info['size']})")
              
              # 初始化scraper用于后续请求
              scraper = get_cf_scraper()
              scraper.cookies.update(cookies)
              
              # 下载所有文件
              for file_info in files:
                  file_id = file_info["file_id"]
                  file_name = file_info["name"]
                  
                  print(f"\n获取文件下载token: {file_name}")
                  token = get_download_token(scraper, cookies, file_id)
                  
                  print(f"构建下载链接: {file_name}")
                  download_url = construct_download_url(file_name, token)
                  
                  print(f"开始下载: {file_name}")
                  download_file(download_url, file_name, cookies)
          
          if __name__ == "__main__":
              main()
          EOF
          
          python xunlei_downloader.py